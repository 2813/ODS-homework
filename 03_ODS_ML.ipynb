{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2813/ODS-homework/blob/main/03_ODS_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "def36360",
      "metadata": {
        "id": "def36360"
      },
      "source": [
        "# **Выбор моделей и работа с признаками**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f5066ff",
      "metadata": {
        "id": "8f5066ff"
      },
      "source": [
        "## **Подготовка для работы в Google Colab или Kaggle**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ebbbf4",
      "metadata": {
        "id": "97ebbbf4"
      },
      "source": [
        "#### Код для подключения Google Drive в Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2b6484",
      "metadata": {
        "id": "6d2b6484"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9798d3",
      "metadata": {
        "id": "ad9798d3"
      },
      "source": [
        "#### Код для получения пути к файлам в Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b2917c8",
      "metadata": {
        "id": "7b2917c8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266cca27",
      "metadata": {
        "id": "266cca27"
      },
      "source": [
        "#### Код для установки библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "223be0c3",
      "metadata": {
        "id": "223be0c3"
      },
      "outputs": [],
      "source": [
        "%pip install numpy==1.26.4 pandas==2.1.4 scikit-learn==1.7.0 statsmodels==0.14.4 matplotlib==3.8.0 seaborn==0.13.2  nltk==3.9.1 missingno==0.5.2 mlxtend==0.23.4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "183c9f42",
      "metadata": {
        "id": "183c9f42"
      },
      "source": [
        "## **Важная информация**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0a84b62",
      "metadata": {
        "id": "e0a84b62"
      },
      "source": [
        "**Для правильного воспроизведения результатов** решения задач:\n",
        "\n",
        "* Рекомендуется придерживаться имеющего в заданиях кода в исходной последовательности. Для этого при решении задач **восстановите недостающие фрагменты кода, которые отмечены символом** `...` (Ellipsis).\n",
        "\n",
        "* Если класс, функция или метод предусматривает параметр random_state, всегда указывайте **random_state=RANDOM_STATE**.\n",
        "\n",
        "* Для всех параметров (кроме random_state) класса, функции или метода **используйте значения по умолчанию, если иное не указано в задании**.\n",
        "\n",
        "**Если скорость обучения слишком низкая**, рекомендуется следующее:\n",
        "\n",
        "* В модели или/и GridSearchCV поменяйте значение параметра n_jobs, который отвечает за параллелизм вычислений.\n",
        "\n",
        "* Воспользуйтесь вычислительными ресурсами Google Colab или Kaggle.\n",
        "\n",
        "***Использовать GPU не рекомендуется, поскольку результаты обучения некоторых моделей могут отличаться на CPU и GPU.***\n",
        "\n",
        "После выполнения каждого задания **ответьте на вопросы в тесте.**\n",
        "\n",
        "**ВНИМАНИЕ:** **После каждого нового запуска ноутбука** перед тем, как приступить к выполнению заданий, проверьте настройку виртуального окружения, выполнив код в ячейке ниже."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3f7499f",
      "metadata": {
        "id": "f3f7499f"
      },
      "outputs": [],
      "source": [
        "# Код для проверки настройки виртуального окружения\n",
        "\n",
        "import sys\n",
        "from importlib.metadata import version\n",
        "\n",
        "required = {\n",
        "    'python': '3.11.x',\n",
        "    'numpy': '1.26.4',\n",
        "    'pandas': '2.1.4',\n",
        "    'scikit-learn': '1.7.0',\n",
        "    'statsmodels': '0.14.4',\n",
        "    'matplotlib': '3.8.0',\n",
        "    'seaborn': '0.13.2',\n",
        "    'nltk': '3.9.1',\n",
        "    'missingno': '0.5.2',\n",
        "    'mlxtend': '0.23.4'\n",
        "}\n",
        "\n",
        "print(f'{\"Компонент\":<15} | {\"Требуется\":<12} | {\"Установлено\":<12} | {\"Соответствие\"}')\n",
        "print('-' * 62)\n",
        "\n",
        "environment_ok = True\n",
        "for lib, req_ver in required.items():\n",
        "    try:\n",
        "        if lib == 'python':\n",
        "            inst_ver = sys.version.split()[0]\n",
        "            status = '✓' if sys.version_info.major == 3 and sys.version_info.minor == 11 else f'x (требуется {req_ver})'\n",
        "        else:\n",
        "            inst_ver = version(lib)\n",
        "            if inst_ver == req_ver:\n",
        "                status = '✓'\n",
        "            else:\n",
        "                environment_ok = False\n",
        "                status = f'x (требуется {req_ver})'\n",
        "    except:\n",
        "        environment_ok = False\n",
        "        inst_ver = '-'\n",
        "        status = 'x (не установлена)'\n",
        "    print(f'{lib:<15} | {req_ver:<12} | {inst_ver:<12} | {status}')\n",
        "\n",
        "print('\\nРезультат проверки: ',\n",
        "      '✓\\nВсе версии соответствуют требованиям'\n",
        "      if environment_ok else\n",
        "      'x\\nВНИМАНИЕ: Версии некоторых компонентов не соответствуют требованиям!\\n'\n",
        "      'Для решения проблемы обратитесь к инструкции по настройке виртуального окружения')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37267d90",
      "metadata": {
        "id": "37267d90"
      },
      "source": [
        "## **Импорт библиотек и вспомогательные функции**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e60646",
      "metadata": {
        "id": "17e60646"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30a39154-71ed-4cde-82ed-06744e845547",
      "metadata": {
        "id": "30a39154-71ed-4cde-82ed-06744e845547"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import missingno as msno\n",
        "\n",
        "from mlxtend.evaluate import bias_variance_decomp\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge\n",
        "from sklearn.metrics import classification_report, roc_auc_score, f1_score, mean_absolute_percentage_error, r2_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from statsmodels.stats.weightstats import ttest_ind\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet, stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a95a346",
      "metadata": {
        "id": "8a95a346"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b282940",
      "metadata": {
        "id": "5b282940"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "488339da",
      "metadata": {
        "id": "488339da"
      },
      "outputs": [],
      "source": [
        "def metrics_report(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Выводит отчёт с основными метриками качества регрессии.\n",
        "    Округляет до 4-х знаков после запятой и выводит значения R2 (коэффициент детерминации), RMSE (среднеквадратичная ошибка) и MAPE (средняя абсолютная процентная ошибка) для оценки качества предсказаний.\n",
        "\n",
        "    Аргументы:\n",
        "        y_true (numpy.ndarray): Истинные значения целевой переменной.\n",
        "        y_pred (numpy.ndarray): Предсказанные значения целевой переменной.\n",
        "    \"\"\"\n",
        "    print(f'R2 score: {r2_score(y_true, y_pred):.4f}')\n",
        "    print(f'RMSE: {mean_squared_error(y_true, y_pred)**0.5:.4f}')\n",
        "    print(f'MAPE: {mean_absolute_percentage_error(y_true, y_pred):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f623678",
      "metadata": {
        "id": "0f623678"
      },
      "source": [
        "### **Обработка пропущенных значений**\n",
        "\n",
        "**Пропуски в данных (NaN, NULL)** — это незаполненные, пустые ячейки в данных, наличие которых влечет за собой множество проблем, включая искажение данных и снижение качества моделей машинного обучения.\n",
        "\n",
        "**Методы обработки пропусков:**\n",
        "\n",
        "* Удаление пропусков:\n",
        "\n",
        "    * Удаление строк — удаление всех строк, содержащих хотя бы один пропуск.\n",
        "\n",
        "    * Удаление столбцов — удаление столбцов (переменных), имеющих большое число пропусков (к примеру, более 50%).\n",
        "\n",
        "* Заполнение константами:\n",
        "\n",
        "    * Заполнение числовыми данными — 0, -1, 9999.\n",
        "\n",
        "    * Заполнение категориальными данными — 'NaN', 'Missing'.\n",
        "\n",
        "* Добавление бинарного признака — индикатора пропуска.\n",
        "\n",
        "* Предсказание пропусков (импутация, imputation):\n",
        "\n",
        "    * Заполнение средним/медианой (числовые данные) или модой (категориальные данные).\n",
        "\n",
        "    * Заполнение случайным значением из распределения.\n",
        "\n",
        "    * Заполнение с помощью прогноза моделей (KNN, Random Forest, MICE).\n",
        "\n",
        "Пропуски в данных могут возникать по разным причинам, и от этого зависит, какой метод обработки будет наиболее эффективным. Выделяют **три основных типа пропусков**:\n",
        "\n",
        "* MCAR (Missing Completely At Random) — пропуски полностью случайны.\n",
        "\n",
        "    * Удаление строк (если пропусков мало).\n",
        "\n",
        "    * Заполнение средним/медианой/модой (если пропусков много).\n",
        "\n",
        "    * Множественная импутация (MICE, KNN, Random Forest).\n",
        "\n",
        "* MAR (Missing At Random) – пропуски случайны, но зависят от других наблюдаемых данных.\n",
        "\n",
        "    * Условное заполнение по группам.\n",
        "\n",
        "    * Добавление бинарного признака.\n",
        "\n",
        "    * Множественная импутация (MICE, KNN, Random Forest).\n",
        "\n",
        "* MNAR (Missing Not At Random) – пропуски неслучайны и зависят от ненаблюдаемых факторов.\n",
        "\n",
        "    * Сбор дополнительных данных (если существует возможность).\n",
        "\n",
        "    * Модели с учётом MNAR (к примеру, Heckman correction).\n",
        "\n",
        "    * Множественная импутация с помощью более сложных моделей (к примеру, Deep Learning с учётом пропусков).\n",
        "\n",
        "*Выше перечислены основные методы обработки пропущенных значений. На практике количество существующих методов больше, а их применение **зависит от задачи***."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16eacf88",
      "metadata": {
        "id": "16eacf88"
      },
      "source": [
        "### **Датасет *Life Expectancy (WHO)***\n",
        "\n",
        "**Для решения заданий 1 — 3 рассмотрим датасет [Life Expectancy (WHO)](https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who).**\n",
        "\n",
        "**ВНИМАНИЕ:** При решении заданий **используйте файл life_expectancy.csv** из приложения к ноутбуку, поскольку исходный датасет был изменен авторами курса.\n",
        "\n",
        "Рассмотрим набор данных для прогнозирования ожидаемой продолжительности жизни в 193-x странах за несколько лет. Датасет предназначен для выявления ключевых демографических, экономических и социальных факторов, которые в наибольшей степени влияют на показатель продолжительности жизни.\n",
        "\n",
        "Одной из особенностей датасета является наличие **большого числа пропущенных значений**, которые необходимо обработать.\n",
        "\n",
        "Целевая переменная — life expectancy (ожидаемая продолжительность жизни в годах).\n",
        "\n",
        "Датасет содержит 2938 наблюдений за период с 2000 по 2015 год для 193-x стран и 18 признаков, включая демографические показатели, показатели смертности, экономические показатели, показатели здоровья и др."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ce8981",
      "metadata": {
        "id": "22ce8981"
      },
      "source": [
        "### ***Задание 1***\n",
        "\n",
        "Из исходного набора данных удалите строки с пропусками в целевой переменной и разделите датасет на обучающую и тестовую выборки так, чтобы **в тестовую выборку вошли все данные за 2012-2015 год**.\n",
        "\n",
        "Выполните анализ пропущенных значений **в обучающей выборке**:\n",
        "\n",
        "1. Посчитайте доли пропусков в каждой переменной с пропусками.\n",
        "\n",
        "2. Выполните визуальный анализ пропущенных значений с помощью библиотеки missingno: [bar](https://github.com/ResidentMario/missingno?tab=readme-ov-file#bar), [matrix](https://github.com/ResidentMario/missingno?tab=readme-ov-file#matrix), [heatmap](https://github.com/ResidentMario/missingno?tab=readme-ov-file#heatmap).\n",
        "\n",
        "3. Выполните статистическую проверку пропусков на MCAR (см. далее).\n",
        "\n",
        "Для статистической проверки пропусков в факторе (predictor) на MCAR (полностью случайные пропуски) предлагается провести t-тест на наличие статистически значимой разницы в среднем значении целевой переменной (life expectancy) в группе с пропущенными значениями и в группе без пропусков.\n",
        "\n",
        "**Процедура проверки:**\n",
        "\n",
        "1. Выбирается признак для проверки на MCAR (predictor).\n",
        "\n",
        "2. Выборка делится на две группы: только с пропущенными значениями predictor и очищенная от всех пропусков predictor.\n",
        "\n",
        "3. Выполняется t-тест на статистическую разницу средних целевой переменной (feature) в двух группах:\n",
        "\n",
        "    * $H_0$: Средние значения feature в обеих группах равны. Пропуски в predictor не связаны со значениями feature (MCAR).\n",
        "\n",
        "    * $H_1$: Средние значения feature в двух группах различаются. Существует связь пропусков в predictor со значениями feature (MAR или MNAR).\n",
        "\n",
        "Реализуйте предложенную процедуру проверки признаков на MCAR, дополнив функцию mcar_test.\n",
        "\n",
        "Для каждого признака с пропусками **на обучающей выборке** выполните статистическую проверку пропущенных значений на принадлежность к типу MCAR (функция mcar_test) **на уровне значимости 1%**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65bdc974",
      "metadata": {
        "id": "65bdc974"
      },
      "outputs": [],
      "source": [
        "# Считайте набор данных\n",
        "\n",
        "df_life = pd.read_csv('life_expectancy.csv')\n",
        "df_life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a59f86fd",
      "metadata": {
        "id": "a59f86fd"
      },
      "outputs": [],
      "source": [
        "# В методе info отображается количество непустых наблюдений для каждого из признаков\n",
        "\n",
        "df_life.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f63d92b",
      "metadata": {
        "id": "3f63d92b"
      },
      "outputs": [],
      "source": [
        "# Создайте списки количественных и категориальных переменных (не включая целевую переменную)\n",
        "# year — количественная переменная\n",
        "\n",
        "life_num_feat = ...\n",
        "life_cat_feat = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b16072",
      "metadata": {
        "id": "42b16072"
      },
      "outputs": [],
      "source": [
        "# Удалите строки с пропусками в целевой переменной\n",
        "\n",
        "df_life = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6b55d8",
      "metadata": {
        "id": "ec6b55d8"
      },
      "outputs": [],
      "source": [
        "# Разделите датасет на обучающую и тестовую выборки так, чтобы в тестовую выборку вошли все данные за 2012-2015 год\n",
        "# Выделять целевой признак в отдельную переменную не требуется\n",
        "\n",
        "life_train = ...\n",
        "life_test = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f642db",
      "metadata": {
        "id": "b9f642db"
      },
      "outputs": [],
      "source": [
        "# Посчитайте доли пропусков в обучающей выборке\n",
        "\n",
        "life_nan_info = pd.DataFrame({'NaN share': ...})\n",
        "life_nan_info = life_nan_info[life_nan_info['NaN share'] != 0.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b655b8e8",
      "metadata": {
        "id": "b655b8e8"
      },
      "source": [
        "#### [bar](https://github.com/ResidentMario/missingno?tab=readme-ov-file#bar)\n",
        "\n",
        "Визуализация количества отсутствующих данных по столбцам в виде столбчатой диаграммы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0766af1c",
      "metadata": {
        "id": "0766af1c"
      },
      "outputs": [],
      "source": [
        "# Выполните визуальный анализ пропущенных значений в обучающей выборке с помощью msno.bar\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "656e10c6",
      "metadata": {
        "id": "656e10c6"
      },
      "source": [
        "#### [matrix](https://github.com/ResidentMario/missingno?tab=readme-ov-file#matrix)\n",
        "\n",
        "Визуализация матрицы пропущенных значений с высокой плотностью. Позволяет выявить закономерности заполненности данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285f7b07",
      "metadata": {
        "id": "285f7b07"
      },
      "outputs": [],
      "source": [
        "# Выполните визуальный анализ пропущенных значений в обучающей выборке с помощью msno.matrix\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbaf2e09",
      "metadata": {
        "id": "fbaf2e09"
      },
      "source": [
        "#### [heatmap](https://github.com/ResidentMario/missingno?tab=readme-ov-file#heatmap)\n",
        "\n",
        "Тепловая карта корреляции отсутствующих данных. Показывает, насколько сильно наличие или отсутствие одной переменной связано с наличием другой. Корреляция принимает значения от -1 (переменные противоположны в заполненности) до 1 (одинаковая заполненность)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ac189e",
      "metadata": {
        "id": "31ac189e"
      },
      "outputs": [],
      "source": [
        "# Выполните визуальный анализ пропущенных значений в обучающей выборке с помощью msno.heatmap\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f615670",
      "metadata": {
        "id": "8f615670"
      },
      "outputs": [],
      "source": [
        "# Дополните функцию mcar_test\n",
        "\n",
        "def mcar_test(data, predictor, feature):\n",
        "    \"\"\"\n",
        "    Выполняет статистическую проверку пропущенных значений на принадлежность к типу MCAR.\n",
        "    Использует t-тест для сравнения распределения feature между группами с пропущенными и существующими значениями predictor.\n",
        "\n",
        "    Аргументы:\n",
        "        data (pandas.DataFrame): Датасет с данными для анализа.\n",
        "        predictor (str): Название столбца в data с пропущенными значениями (целевая переменная для теста MCAR).\n",
        "        feature (str): Название признака, по которому сравниваются распределения между группами.\n",
        "\n",
        "    Возвращает:\n",
        "        float: p-value t-теста.\n",
        "    \"\"\"\n",
        "    data_nans_in_feature = ...      # Все наблюдения в data с пропусками в predictor\n",
        "    data_non_nans_in_feature = ...  # Все наблюдения в data без пропусков в predictor\n",
        "    _, p_val, _ = ttest_ind(data_nans_in_feature[feature], data_non_nans_in_feature[feature])\n",
        "    return p_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a86274de",
      "metadata": {
        "id": "a86274de"
      },
      "outputs": [],
      "source": [
        "# Используя функцию mcar_test, выполните проверку пропущенных значений на принадлежность к типу MCAR\n",
        "# Используйте только обучающую выборку\n",
        "# Уровень значимости — 1%\n",
        "\n",
        "p_values = []\n",
        "\n",
        "...\n",
        "\n",
        "life_nan_info['p-value'] = p_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34e8806f",
      "metadata": {
        "id": "34e8806f"
      },
      "outputs": [],
      "source": [
        "# Визуализируйте плотность целевой переменной\n",
        "# График позволяет убедиться, что пропуски типов MAR и MNAR зависят от наблюдаемых данных\n",
        "\n",
        "predictor = 'BMI'\n",
        "feature = 'life expectancy'\n",
        "\n",
        "life_train_nans_in_feature = ...            # Все наблюдения в life_train с пропусками в predictor\n",
        "life_train_non_nans_in_feature = ...        # Все наблюдения в life_train без пропусков в predictor\n",
        "\n",
        "dplt = sns.kdeplot(life_train_nans_in_feature[feature], color='red', label=f'В группе с пропусками в {predictor}')\n",
        "dplt = sns.kdeplot(life_train_non_nans_in_feature[feature], color='blue', label=f'В группе без пропусков в {predictor}')\n",
        "dplt.set_title(f'Плотность распределения {feature}')\n",
        "plt.legend()\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c970a93",
      "metadata": {
        "id": "6c970a93"
      },
      "source": [
        "### ***Задание 2***\n",
        "\n",
        "**ВНИМАНИЕ:** Для решения этого задания используйте:\n",
        "\n",
        "* Обучающую и тестовую выборки из задания 1: `life_train`, `life_test`.\n",
        "\n",
        "* Датасет с результатами анализа пропущенных значений в обучающей выборке из задания 1: `life_nan_info`.\n",
        "\n",
        "*Для данного задания предположим, что со временем среднее и дисперсия данных не изменились. Это позволит использовать одни и те же StandardScaler, SimpleImputer и KNNImputer для обучающей и тестовой выборок.*\n",
        "\n",
        "Выполните предобработку данных (см. код задания).\n",
        "\n",
        "Используя результаты анализа пропущенных значений в обучающей выборке (`life_nan_info`), выполните обработку пропусков **в обучающей выборке** (`life_train`) по следующим правилам:\n",
        "\n",
        "* Для всех признаков MCAR (если есть):\n",
        "\n",
        "    * Заполните пропуски средним, обучив SimpleImputer (`life_mean_imputer`).\n",
        "\n",
        "* Для всех признаков MAR или MNAR (если есть):\n",
        "\n",
        "    * Если доля пропусков меньше 5%, заполните пропуски средним, обучив SimpleImputer (`life_mean_imputer`).\n",
        "\n",
        "    * Если доля пропусков больше 5%, заполните пропуски с помощью метода k-ближайших соседей (KNN, K-Nearest Neighbors), обучив KNNImputer (`life_knn_imputer`).\n",
        "\n",
        "Заполните пропуски в **тестовой выборке**, используя обученные на обучающей выборке SimpleImputer (`life_mean_imputer`) и KNNImputer (`life_knn_imputer`) для соответствующих признаков.\n",
        "\n",
        "После обработки пропусков обучите модель линейной регрессии без регуляризации `reg_life` и выведите metrics_report на тестовой выборке.\n",
        "\n",
        "*Pipeline из библиотеки sklearn позволяет использовать SimpleImputer или KNNImputer как трансформатор, то есть как один из этапов предобработки данных в рамках пайплайна*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d64b56fc",
      "metadata": {
        "id": "d64b56fc"
      },
      "outputs": [],
      "source": [
        "# Выделите объясняемый фактор в отдельную переменную\n",
        "\n",
        "X_life_train, y_life_train = ...\n",
        "X_life_test, y_life_test = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695b78a2",
      "metadata": {
        "id": "695b78a2"
      },
      "outputs": [],
      "source": [
        "# Закодируйте категориальные признаки числами 0 и 1 с помощью OneHotEncoder\n",
        "#   train -> fit_transform\n",
        "#   test -> transform\n",
        "\n",
        "life_encoder = OneHotEncoder(sparse_output=False, drop='first').set_output(transform='pandas')\n",
        "\n",
        "X_life_train = ...\n",
        "X_life_test = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e31d24c",
      "metadata": {
        "id": "1e31d24c"
      },
      "outputs": [],
      "source": [
        "# Масштабируйте количественные признаки\n",
        "#   train -> fit_transform\n",
        "#   test -> transform\n",
        "\n",
        "life_scaler = StandardScaler().set_output(transform='pandas')\n",
        "\n",
        "X_life_train_scaled = ...\n",
        "X_life_test_scaled = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5056f0",
      "metadata": {
        "id": "8f5056f0"
      },
      "outputs": [],
      "source": [
        "# Обучите и примените life_mean_imputer (SimpleImputer) к обучающей выборке\n",
        "#   train -> fit_transform\n",
        "\n",
        "life_mean_imputer_feat = ...\n",
        "\n",
        "life_mean_imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "X_life_train_scaled_imputed = X_life_train_scaled.copy()\n",
        "X_life_train_scaled_imputed[life_mean_imputer_feat] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56ed0025",
      "metadata": {
        "id": "56ed0025"
      },
      "outputs": [],
      "source": [
        "# Обучите и примените life_knn_imputer (KNNImputer) к обучающей выборке\n",
        "#   train -> fit_transform\n",
        "\n",
        "life_knn_imputer_feat = ...\n",
        "\n",
        "life_knn_imputer = KNNImputer()\n",
        "\n",
        "X_life_train_scaled_imputed[life_knn_imputer_feat] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c8d6e51",
      "metadata": {
        "id": "7c8d6e51"
      },
      "outputs": [],
      "source": [
        "# Примените life_mean_imputer и life_knn_imputer к тестовой выборке\n",
        "#   test -> transform\n",
        "\n",
        "X_life_test_scaled_imputed = X_life_test_scaled.copy()\n",
        "X_life_test_scaled_imputed[life_mean_imputer_feat] = ...\n",
        "X_life_test_scaled_imputed[life_knn_imputer_feat] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cab5c72",
      "metadata": {
        "id": "2cab5c72"
      },
      "outputs": [],
      "source": [
        "# Обучите reg_life и выведите metrics_report на тестовой выборке\n",
        "\n",
        "reg_life = ...\n",
        "metrics_report(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f3419fc",
      "metadata": {
        "id": "3f3419fc"
      },
      "source": [
        "### **Смещение и разброс**\n",
        "\n",
        "* **Смещение (Bias)** – ожидаемое отклонение предсказаний модели от истинных значений.\n",
        "\n",
        "* **Разброс (Variance)** – это ошибка, вызванная чувствительностью модели к небольшим изменениям в обучающих данных.\n",
        "\n",
        "Смещение и разброс отражают два источника ошибки модели, и между ними существует компромисс: снижение смещения часто ведёт к увеличению разброса, и наоборот. Оптимальная модель должна находить баланс между смещением и разбросом, чтобы минимизировать общую ошибку предсказания.\n",
        "\n",
        "Для анализа ошибки алгоритма применяется **разложение MSE на смещение и разброс (bias-variance decomposition)**.\n",
        "\n",
        "Пусть $y$ – истинные значения целевой переменой. Мы предполагаем, что существует функция $f(x)=y+e$, где $e$ – ошибка с $\\mathbb{E}[e]=0$ (мат. ожидание) и $\\mathbb{D}[e]=\\sigma^2$ (дисперсия). $\\widehat{f}(x)$ – значения целевой переменной, предсказанные моделью.\n",
        "\n",
        "Тогда\n",
        "\n",
        "$$\\text{MSE}=\\mathbb{E}[(y-\\widehat{f}(x))^2]=\\text{Bias}^2(\\widehat{f}(x))+\\text{Variance}(\\widehat{f}(x))+\\sigma^2$$\n",
        "где\n",
        "$$\\text{Bias}^2(\\widehat{f}(x))=(\\mathbb{E}[\\widehat{f}(x)]-f(x))^2$$\n",
        "$$\\text{Variance}(\\widehat{f}(x))=\\mathbb{D}[\\widehat{f}(x)]=\\mathbb{E}[(\\mathbb{E}[\\widehat{y}]-\\widehat{y})^2]$$\n",
        "$\\sigma^2$ — неустранимая ошибка измерения\n",
        "\n",
        "Подробнее можно изучить по **ссылкам:**\n",
        "\n",
        "* [Bias-variance decomposition | education.yandex.ru](https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition).\n",
        "\n",
        "* [Bias-variance decomposition for classification and regression losses | rasbt.github.io](https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfb46465-7dd1-467c-9076-919bc3f4cc06",
      "metadata": {
        "id": "cfb46465-7dd1-467c-9076-919bc3f4cc06"
      },
      "source": [
        "### ***Задание 3***\n",
        "\n",
        "**ВНИМАНИЕ:** Для решения этого задания используйте:\n",
        "\n",
        "* Обучающую и тестовую выборки (после заполнения пропусков) из задания 2: `X_life_train_scaled_imputed`, `X_life_test_scaled_imputed`, `y_life_train`, `y_life_test`.\n",
        "\n",
        "* Обученную модель из задания 2: `reg_life`.\n",
        "\n",
        "Обучите две модели линейной регрессии с регуляризацией:\n",
        "\n",
        "* `lasso_life` — регрессия с L1-регуляризацией (LASSO). Оптимальные гиперпараметры обучения подберите с помощью GridSearchCV.\n",
        "\n",
        "* `ridge_life` — регрессия с L2-регуляризацией (Ridge). Оптимальные гиперпараметры обучения подберите с помощью GridSearchCV.\n",
        "\n",
        "Разложите среднеквадратическую ошибку (MSE) каждой из трех обученных моделей (`reg_life`, `lasso_life`, `ridge_life`) на компоненты (функция [bias_variance_decomp](https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/#api)):\n",
        "\n",
        "* Смещение ($\\text{Bias}^2$).\n",
        "\n",
        "* Разброс ($\\text{Variance}$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aa6996c",
      "metadata": {
        "id": "4aa6996c"
      },
      "outputs": [],
      "source": [
        "# Выполните разложение MSE модели reg_life на смещение и разброс\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "reg_life_mse, reg_life_bias, reg_life_var = bias_variance_decomp(\n",
        "    ...\n",
        "    loss='mse',\n",
        "    num_rounds=100,\n",
        "    random_seed=RANDOM_STATE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f28328fc",
      "metadata": {
        "id": "f28328fc"
      },
      "outputs": [],
      "source": [
        "# Обучите модель lasso_life (LASSO)\n",
        "# Оптимальные гиперпараметры обучения подберите с помощью GridSearchCV\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "params = {'alpha' : [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]}\n",
        "cv = 5\n",
        "\n",
        "cv_lasso_life = ...\n",
        "lasso_life = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b7efcfb",
      "metadata": {
        "id": "2b7efcfb"
      },
      "outputs": [],
      "source": [
        "# Выполните разложение MSE модели lasso_life на смещение и разброс\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "lasso_life_mse, lasso_life_bias, lasso_life_var = bias_variance_decomp(\n",
        "    ...\n",
        "    loss='mse',\n",
        "    num_rounds=100,\n",
        "    random_seed=RANDOM_STATE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce966838",
      "metadata": {
        "id": "ce966838"
      },
      "outputs": [],
      "source": [
        "# Обучите модель ridge_life (Ridge)\n",
        "# Оптимальные гиперпараметры обучения подберите с помощью GridSearchCV\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "params = {'alpha' : [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]}\n",
        "cv = 5\n",
        "\n",
        "cv_ridge_life = ...\n",
        "ridge_life = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb3dc491",
      "metadata": {
        "id": "eb3dc491"
      },
      "outputs": [],
      "source": [
        "# Выполните разложение MSE модели ridge_life на смещение и разброс\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "ridge_life_mse, ridge_life_bias, ridge_life_var = bias_variance_decomp(\n",
        "    ...\n",
        "    loss='mse',\n",
        "    num_rounds=100,\n",
        "    random_seed=RANDOM_STATE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "632ed4f2",
      "metadata": {
        "id": "632ed4f2"
      },
      "source": [
        "## **Извлечение признаков из текста**\n",
        "\n",
        "**Классификация тональности (sentiment analysis)** — это задача определения эмоциональной окраски текста в рамках NLP (Natural Language Processing).\n",
        "\n",
        "Как правило, тональность текста делят на три категории:\n",
        "\n",
        "* Позитивная (к примеру, положительный отзыв).\n",
        "\n",
        "* Нейтральная (информация без явной эмоциональной окраски).\n",
        "\n",
        "* Негативная (к примеру, критика или жалоба).\n",
        "\n",
        "Перед обучением моделей необходимо извлечь признаки из текстовых данных, проведя предобработку текста.\n",
        "\n",
        "Основные этапы предобработки текста:\n",
        "\n",
        "1. Очистка текста от лишних элементов (не влияющих на тональность):\n",
        "\n",
        "    * Удаление спецсимволов — удаление HTML-тегов, URL, email, хештегов, упоминаний (@user).\n",
        "\n",
        "    * Удаление цифр — удаление или замена на числительные.\n",
        "\n",
        "    * Удаление пунктуации.\n",
        "\n",
        "    * Удаление лишних пробелов и дублирующихся  символов.\n",
        "\n",
        "2. Токенизация — разбиение текста на отдельные токены (слова, части слов или символы). Пример: 'Я люблю этот фильм!' $\\to$ ['Я', 'люблю', 'этот', 'фильм', '!'].\n",
        "\n",
        "3. Нормализация слов:\n",
        "\n",
        "    * Лемматизация — приведение слова к лемме — её нормальной (словарной) форме. Пример: 'бежал' $\\to$ 'бежать'.\n",
        "\n",
        "    * Стемминг — выделение основы слова. Пример: 'бежал' $\\to$ 'беж'.\n",
        "\n",
        "4. Удаление стоп-слов — исключение слов, которые часто встречаются в тексте, но не несут значимой смысловой нагрузки для анализа тональности: местоимения, предлоги, союзы, частицы, артикли (для английского) и т.д.\n",
        "\n",
        "5. Векторизация текста — преобразование текста в числовой формат для обучения и использования моделей машинного обучения:\n",
        "\n",
        "    * BoW (Bag-of-words).\n",
        "\n",
        "    * TF-IDF (Term Frequency — Inverse Document Frequency).\n",
        "\n",
        "    * Векторные эмбеддинги: FastText, Word2Vec, GloVe.\n",
        "\n",
        "    * Контекстные эмбеддинги: BERT.\n",
        "\n",
        "*Это лишь основные этапы предобработки текста. На практике количество существующих методов обработки данных больше, а структура и порядок этапов **зависит от задачи***.\n",
        "\n",
        "Подробнее можно изучить по **ссылкам:**\n",
        "\n",
        "* [Основы Natural Language Processing для текста | habr.com](https://habr.com/ru/companies/Voximplant/articles/446738/)\n",
        "\n",
        "* [Преобразование текстовых данных и работа с ними в Python | education.yandex.ru](https://education.yandex.ru/handbook/data-analysis/article/preobrazovanie-tekstovyh-dannyh-i-rabota-s-nimi-v-python)\n",
        "\n",
        "* [Краткий обзор техник векторизации в NLP | habr.com](https://habr.com/ru/articles/778048/)\n",
        "\n",
        "* [How to Create Bert Vector Embeddings? A Comprehensive Tutorial | airbyte.com](https://airbyte.com/data-engineering-resources/bert-vector-embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cfae23d",
      "metadata": {
        "id": "1cfae23d"
      },
      "source": [
        "### **TF-IDF**\n",
        "\n",
        "**TF-IDF** (Term Frequency — Inverse Document Frequency) — это статистическая мера, для оценки важности слова в контексте документа (набора текстовых данных), являющегося частью коллекции или корпуса документов. Высокий вес по TF-IDF получают слова, которые часто встречаются в конкретном документе, но при этом редко встречаются во всех остальных документах корпуса.\n",
        "\n",
        "Эта мера состоит из двух компонентов:\n",
        "\n",
        "* TF (Term Frequency — частота слова). TF измеряет, насколько часто слово встречается в конкретном документе. Чем чаще слово появляется в тексте, тем выше его TF.\n",
        "\n",
        "$$\\text{TF} = \\frac{\\text{Количество раз, когда слово встретилось в документе}}{\\text{Общее количество слов в документе}}$$\n",
        "\n",
        "* IDF (Inverse Document Frequency — обратная документная частота). IDF измеряет уникальность или информативность слова в масштабах всего корпуса документов. Компонент IDF уменьшает вес слов, которые встречаются слишком часто во многих документах (например, предлоги, союзы). Если слово редкое, его IDF будет высоким.\n",
        "\n",
        "$$\\text{IDF} = \\log(\\frac{\\text{Общее количество документов в корпусе}}{\\text{Количество документов, в которых встретилось слово} + 1})$$\n",
        "\n",
        "Итоговая формула TF-IDF:\n",
        "\n",
        "$$\\text{TF-IDF}=\\text{TF} \\times \\text{IDF}$$\n",
        "\n",
        "Таким образом, слово получает высокий вес TF-IDF, если оно:\n",
        "\n",
        "* Часто встречается в данном документе (высокий TF).\n",
        "\n",
        "* Редко встречается в других документах корпуса (высокий IDF).\n",
        "\n",
        "TF-IDF позволяет преобразовать наборы текстовых данных в разреженную матрицу признаков, которая может быть использована в алгоритмах машинного обучения.\n",
        "\n",
        "**Процедура преобразования:**\n",
        "\n",
        "1. Создание словаря. Составляется полный список всех уникальных слов, которые встретились во всех текстах из корпуса. Каждому уникальному слову присваивается индекс (номер столбца в матрице).\n",
        "\n",
        "2. Для каждого документа (строки) вычисляется TF-IDF вес каждого слова из словаря. Если слово из общего словаря отсутствует в данном документе, его вес для данного документа будет равен 0.\n",
        "\n",
        "Итоговая матрица признаков, используемая для обучения моделей ML:\n",
        "\n",
        "* Строки — документы.\n",
        "\n",
        "* Столбцы — уникальные слова из всего корпуса документов.\n",
        "\n",
        "* Значения в ячейках — TF-IDF веса слов."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc95b033-a261-46ef-8fa0-2b356e057f46",
      "metadata": {
        "id": "fc95b033-a261-46ef-8fa0-2b356e057f46"
      },
      "source": [
        "### ***Задание 4***\n",
        "\n",
        "Рассмотрим упрощенный пример бинарной классификации тональности текста с помощью логистической регрессии.\n",
        "\n",
        "Выполните предобработку текстов:\n",
        "\n",
        "1. Приведите все тексты к нижнему регистру (lowercase). *Подсказка: изучите [векторизованные строковые функции pandas](https://pandas.pydata.org/docs/user_guide/text.html#method-summary).*\n",
        "\n",
        "2. Очистите тексты от ссылок и лишних символов с помощью функции regex_clean.\n",
        "\n",
        "3. Дополните функцию `tokenize` и токенизируйте тексты.\n",
        "\n",
        "4. Дополните функцию `pos_lemmatize` и лемматизируйте тексты c учетом частей речи слов (POS — Part Of Speech). *Подробнее по ссылке: [Lemmatization in NLP | medium.com](https://medium.com/@kevinnjagi83/lemmatization-in-nlp-2a61012c5d66).*\n",
        "\n",
        "5. Дополните функцию `remove_stop_words` и удалите из текстов стоп-слова.\n",
        "\n",
        "6. Соберите список лемм (после удаления стоп-слов) в одну строку, отделив леммы знаком пробела. *Подсказка: изучите [векторизованные строковые функции pandas](https://pandas.pydata.org/docs/user_guide/text.html#method-summary).*\n",
        "\n",
        "7. Разделите датасет на обучающую (60%) и тестовую (40%) выборки со стратификацией по целевой переменной.\n",
        "\n",
        "Постройте пайплайн `twit_pipeline`:\n",
        "\n",
        "1. 'tfidf': [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
        "\n",
        "2. 'clf': LogisticRegression.\n",
        "\n",
        "Используя пайплайн `twit_pipeline`, подберите оптимальные гиперпараметры обучения с помощью GridSearchCV **(метрика оптимизации — AUC)**, и с оптимальными гиперпараметрами обучите модель `lr_twit` для бинарной классификации тональности текстов.\n",
        "\n",
        "Для модели `lr_twit` на тестовой выборке постройте отчет по метрикам классификации и посчитайте метрику AUC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30787d78-f6b7-438e-a2be-6cc51d2da860",
      "metadata": {
        "id": "30787d78-f6b7-438e-a2be-6cc51d2da860"
      },
      "outputs": [],
      "source": [
        "# Считайте данные\n",
        "\n",
        "df_twit = pd.read_csv('tweets.csv')\n",
        "df_twit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "815ce768",
      "metadata": {
        "id": "815ce768"
      },
      "outputs": [],
      "source": [
        "# Удалите из датасета нейтральную ('neutral') тональность\n",
        "\n",
        "df_twit = df_twit[df_twit['sentiment'] != 'neutral']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35fd9f72",
      "metadata": {
        "id": "35fd9f72"
      },
      "outputs": [],
      "source": [
        "# Закодируйте целевую переменную:\n",
        "#   0 — 'negative'\n",
        "#   1 — 'positive'\n",
        "# Сбросьте индексы в новом датасете\n",
        "\n",
        "df_twit['sentiment'] = ...\n",
        "df_twit = df_twit.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a8c722",
      "metadata": {
        "id": "c3a8c722"
      },
      "outputs": [],
      "source": [
        "# Выведите исходный текст с индексом 1901\n",
        "\n",
        "print(df_twit.iloc[1901]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eedd6aae",
      "metadata": {
        "id": "eedd6aae"
      },
      "outputs": [],
      "source": [
        "# Приведите все тексты к нижнему регистру (lowercase)\n",
        "\n",
        "df_twit['text lowercase'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986f1170",
      "metadata": {
        "id": "986f1170"
      },
      "outputs": [],
      "source": [
        "# Выведите текст с индексом 1901 после приведения к нижнему регистру\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30cb4e34",
      "metadata": {
        "id": "30cb4e34"
      },
      "outputs": [],
      "source": [
        "def regex_clean(text):\n",
        "    \"\"\"\n",
        "    Очищает текст от ссылок и лишних символов с помощью регулярных выражений.\n",
        "\n",
        "    Аргументы:\n",
        "        text (str): Входной текст для очистки.\n",
        "\n",
        "    Возвращает:\n",
        "        str: Очищенный текст.\n",
        "    \"\"\"\n",
        "    # Замена всех ссылок в тексте на пробел\n",
        "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n",
        "\n",
        "    # Удаление всех цифр, спецсимволов и знаков пунктуации\n",
        "    text = re.sub('[0-9!#()$\\,\\'\\-\\.*+/:;<=>?@[\\]^_`{|}\\\"]+', ' ', text)\n",
        "\n",
        "    # Удаление лишних пробелов\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92876064",
      "metadata": {
        "id": "92876064"
      },
      "outputs": [],
      "source": [
        "# Очистите тексты от ссылок и лишних символов с помощью функции regex_clean\n",
        "\n",
        "df_twit['text cleaned'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d3879ec",
      "metadata": {
        "id": "1d3879ec"
      },
      "outputs": [],
      "source": [
        "# Выведите текст с индексом 1901 после удаления ссылок и лишних символов\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71456c8b",
      "metadata": {
        "id": "71456c8b"
      },
      "outputs": [],
      "source": [
        "# Дополните функцию tokenize\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Токенизирует текст на отдельные слова или токены с использованием NLTK.\n",
        "    Для токенизации используется функиця word_tokenize.\n",
        "\n",
        "    Аргументы:\n",
        "        text (str): Входной текст для токенизации.\n",
        "\n",
        "    Возвращает:\n",
        "        list(str): Список токенов, полученных в результате разбиения текста.\n",
        "    \"\"\"\n",
        "    return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e209251a",
      "metadata": {
        "id": "e209251a"
      },
      "outputs": [],
      "source": [
        "# Токенизируйте тексты с помощью tokenize\n",
        "\n",
        "df_twit['text tokenized'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5ff25ef",
      "metadata": {
        "id": "e5ff25ef"
      },
      "outputs": [],
      "source": [
        "# Выведите текст с индексом 1901 после токенизации\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d21a84",
      "metadata": {
        "id": "93d21a84"
      },
      "outputs": [],
      "source": [
        "def treebank_to_wordnet(treebank_pos_tag):\n",
        "    \"\"\"\n",
        "    Преобразует POS-теги из формата Penn Treebank в формат WordNet.\n",
        "\n",
        "    Аргументы:\n",
        "        treebank_pos_tag (str): POS-тег в формате Penn Treebank.\n",
        "\n",
        "    Возвращает:\n",
        "        object: POS-тег в формате WordNet. В случае неизвестного тега по умолчанию возвращает wordnet.NOUN (существительное).\n",
        "    \"\"\"\n",
        "    if treebank_pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_pos_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0fd1490",
      "metadata": {
        "id": "c0fd1490"
      },
      "outputs": [],
      "source": [
        "# Дополните функцию pos_lemmatize\n",
        "\n",
        "def pos_lemmatize(tokens):\n",
        "    \"\"\"\n",
        "    Лемматизирует список токенов, преобразуя слова к их базовой форме с учетом части речи (POS).\n",
        "    Для лемматизации используется WordNetLemmatizer.\n",
        "\n",
        "    Аргументы:\n",
        "        tokens (list(str)): Список токенов для лемматизации.\n",
        "\n",
        "    Возвращает:\n",
        "        list(str): Список лемм, полученных в результате лемматизации токенов.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Определение POS-тегов в формате Penn Treebank для каждого токена\n",
        "    pos_tokens = pos_tag(...)\n",
        "\n",
        "    # Лемматизируйте каждый токен с учетом его POS-тега\n",
        "    # POS-теги в формате Penn Treebank необходимо преобразовать в формат WordNet (используйте функцию treebank_to_wordnet)\n",
        "    lemmas = [lemmatizer.lemmatize(...) for token, pos in ...]\n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a469a630",
      "metadata": {
        "id": "a469a630"
      },
      "outputs": [],
      "source": [
        "# Лемматизируйте тексты с помощью pos_lemmatize\n",
        "\n",
        "df_twit['text lemmatized'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d181ee5",
      "metadata": {
        "id": "2d181ee5"
      },
      "outputs": [],
      "source": [
        "# Выведите текст с индексом 1901 после лемматизации\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3ba891",
      "metadata": {
        "id": "9a3ba891"
      },
      "outputs": [],
      "source": [
        "# Дополните функцию remove_stop_words\n",
        "\n",
        "def remove_stop_words(lemmas, stop_words):\n",
        "    \"\"\"\n",
        "    Удаляет стоп-слова из списка лемм.\n",
        "\n",
        "    Аргументы:\n",
        "        lemmas (list(str)): Список лемм.\n",
        "        stop_words (list(str)): Список стоп-слов для фильтрации.\n",
        "\n",
        "    Возвращает:\n",
        "        list(str): Список лемм после удаления стоп-слов.\n",
        "    \"\"\"\n",
        "    lemmas = ...\n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eee4026",
      "metadata": {
        "id": "1eee4026"
      },
      "outputs": [],
      "source": [
        "# Удалите стоп-слова из текстов с помощью remove_stop_words\n",
        "# В качестве списка стоп-слов используйте список из библиотеки NLTK\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "df_twit['text without stop words'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "777f85b6",
      "metadata": {
        "id": "777f85b6"
      },
      "outputs": [],
      "source": [
        "# Соберите список лемм в одну строку, отделив леммы знаком пробела\n",
        "\n",
        "df_twit['text preprocessed'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be16a81",
      "metadata": {
        "id": "6be16a81"
      },
      "outputs": [],
      "source": [
        "# Выведите текст с индексом 1901 после удаления стоп-слов\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62ca3bf9",
      "metadata": {
        "id": "62ca3bf9"
      },
      "outputs": [],
      "source": [
        "# Выделите объясняемый фактор в отдельную переменную\n",
        "\n",
        "X_twit, y_twit = df_twit['text preprocessed'], df_twit['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3297959-ae8e-46e7-a184-a61be39472c8",
      "metadata": {
        "id": "a3297959-ae8e-46e7-a184-a61be39472c8"
      },
      "outputs": [],
      "source": [
        "# Разделите датасет на обучающую (60%) и тестовую (40%) выборки со стратификацией по целевой переменной\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "X_twit_train, X_twit_test, y_twit_train, y_twit_test = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed24a2b",
      "metadata": {
        "id": "6ed24a2b"
      },
      "outputs": [],
      "source": [
        "# Постройте пайплайн twit_pipeline\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "twit_pipeline = Pipeline([\n",
        "    ('tfidf', ...),\n",
        "    ('clf', ...)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a17b379",
      "metadata": {
        "id": "0a17b379"
      },
      "outputs": [],
      "source": [
        "# Обучите модель lr_twit на обучающих данных\n",
        "# Оптимальные гиперпараметры обучения подберите с помощью GridSearchCV\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "params = {\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2)], # (1, 1) — только отдельные слова / (1, 2) — слова и биграммамы (пары слов)\n",
        "    'tfidf__max_df': [0.9, 0.95],           # Верхний порог частоты слова в корпусе текстов (если частота выше, слово игнорируется)\n",
        "    'clf__C': [0.001, 0.1, 1.0, 10.0]       # Параметр регуляризации для логистической регрессии\n",
        "}\n",
        "scoring = 'roc_auc'\n",
        "cv = 5\n",
        "\n",
        "cv_lr_twit = GridSearchCV(\n",
        "    estimator=...,\n",
        "    param_grid=...,\n",
        "    scoring=...,\n",
        "    cv=...,\n",
        "    n_jobs=-1                               # Используйте все доступные ядра CPU\n",
        ").fit(...)\n",
        "\n",
        "lr_twit = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2c80ba9",
      "metadata": {
        "id": "f2c80ba9"
      },
      "outputs": [],
      "source": [
        "cv_lr_twit.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3936e3-3e54-44f8-847a-ade35237d6df",
      "metadata": {
        "id": "aa3936e3-3e54-44f8-847a-ade35237d6df"
      },
      "outputs": [],
      "source": [
        "# Постройте отчет по метрикам классификации на тестовой выборке для lr_twit\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "641b0ccd",
      "metadata": {
        "id": "641b0ccd"
      },
      "outputs": [],
      "source": [
        "# Рассчитайте метрику AUC на тестовой выборке для lr_twit\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b22c745d",
      "metadata": {
        "id": "b22c745d"
      },
      "source": [
        "### **Извлечение признаков из временных рядов**\n",
        "\n",
        "Целью извлечения признаков из временных рядов является преобразование исходного временного ряда (последовательности точек во времени) в набор информативных и компактных статических признаков.\n",
        "\n",
        "**Основные типы временных признаков:**\n",
        "\n",
        "* Признаки на основе лагов (предыдущих значений ряда).\n",
        "\n",
        "    * Лаговые признаки — это значения временного ряда из предыдущих моментов времени.\n",
        "\n",
        "    * Скользящее среднее (moving average) — среднее значение за несколько предыдущих периодов.\n",
        "\n",
        "* Признаки на основе даты и времени.\n",
        "\n",
        "    * Временные компоненты: час, день недели, день месяца, месяц, год и т.д.\n",
        "\n",
        "    * Бинарные флаги: является ли день выходным, праздничным и т.д.\n",
        "\n",
        "* Циклические признаки. Чтобы сохранить информацию о цикличности, признаки преобразуют в двумерное пространство с помощью синуса и косинуса. Для признака со значением $x$ и периодом $T$ (например, для часа $T = 24$, для дня недели $T = 7$) создаются два новых признака:\n",
        "\n",
        "    $$x_{\\text{cos}} = \\text{cos}(\\frac{2 \\pi x}{T})$$\n",
        "\n",
        "    $$x_{\\text{sin}} = \\text{sin}(\\frac{2 \\pi x}{T})$$\n",
        "\n",
        "*Это лишь основные и наиболее простые типы временных признаков. На практике их количество больше, а использование того или иного типа **зависит от решаемой задачи***."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fe7d31",
      "metadata": {
        "id": "06fe7d31"
      },
      "source": [
        "### **Кросс-валидация временных рядов**\n",
        "\n",
        "Классические методы кросс-валидации не могут быть использованы для временных рядов, упорядоченных во времени. Это связано с проблемой утечки данных (data leakage): случайное перемешивание временного ряда приведет к тому, что модель будет обучаться на данных из будущего, чтобы предсказать прошлое.\n",
        "\n",
        "Для временных рядов необходимы особые стратегии кросс-валидации, которые сохраняют временной порядок данных.\n",
        "\n",
        "**[TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)** — это итератор кросс-валидации временных рядов из библиотеки scikit-learn. TimeSeriesSplit реализует подход, известный как кросс-валидация на расширяющемся окне (expanding window cross-validation): c каждой новой итерацией (фолдом) обучающая выборка увеличивается, а валидационная сдвигается вперед во времени.\n",
        "\n",
        "TimeSeriesSplit может быть использован для подбора гиперпараметров с помощью GridSearchCV. Если **в качестве параметра cv** в GridSearchCV использовать разбиение временного ряда, полученное с помощью TimeSeriesSplit, GridSearchCV будет обучать и валидировать модель на соответствующих разбиениях (фолдах), обеспечивая корректную оценку качества модели."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef46339",
      "metadata": {
        "id": "4ef46339"
      },
      "source": [
        "### **Датасет *Hourly Energy Consumption***\n",
        "\n",
        "**Для решения задания 5 рассмотрим датасет [Hourly Energy Consumption](https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption).**\n",
        "\n",
        "Набор данных предназначен для анализа и прогнозирования почасового потребления электроэнергии в мегаваттах (MW) в различных регионах, входящих в PJM Interconnection LLC — региональную организацию по передаче электроэнергии в восточной части США.\n",
        "\n",
        "Целевая переменная — AEP_MW (количество потребляемой электроэнергии в мегаваттах за каждый час).\n",
        "\n",
        "Единственная известная переменная датасета — Datetime (временная отметка с точностью до часа, которая указывает дату и время записи)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b170bba1",
      "metadata": {
        "id": "b170bba1"
      },
      "source": [
        "### ***Задание 5***\n",
        "\n",
        "Используя значения ряда (AEP_MW), создайте признаки:\n",
        "\n",
        "* Лаговые признаки (*подсказка: используйте [shift](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html)*):\n",
        "\n",
        "    * lag 1h — значение AEP_MW 1 час назад (временной лаг в 1 наблюдение).\n",
        "\n",
        "    * lag 24h — значение AEP_MW 24 часа назад (временной лаг в 24 наблюдения).\n",
        "\n",
        "* Скользящее среднее (*подсказка: используйте признак **lag 1h** (скользящее среднее строится по **предыдущим** наблюдениям и не должно включать значение целевой переменной на момент прогноза) и [rolling](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html)*):\n",
        "\n",
        "    * mean 24h — среднее AEP_MW за **предыдущие** 24 часа (скользящее среднее по 24 **предыдущим** наблюдениям).\n",
        "\n",
        "    * mean 7d — среднее AEP_MW за **предыдущие** 7 дней (скользящее среднее по 24 $\\times$ 7 **предыдущим** наблюдениям).\n",
        "\n",
        "Используя метку времени (Datetime), создайте признаки:\n",
        "\n",
        "* Временные компоненты:\n",
        "\n",
        "    * hour: час.\n",
        "\n",
        "    * weekday: день недели.\n",
        "\n",
        "    * day: число месяца.\n",
        "\n",
        "    * month: месяц.\n",
        "\n",
        "    * year: год.\n",
        "\n",
        "* Бинарные флаги:\n",
        "\n",
        "    * is_weekend: метка выходного дня (суббота и воскресенье).\n",
        "\n",
        "* Циклические признаки, закодированные с помощью косинуса и синуса:\n",
        "\n",
        "    * hour_cos, hour_sin: cos и sin от hour ($T = 24$).\n",
        "\n",
        "    * day_cos, day_sin: cos и sin от day ($T = 30$).\n",
        "\n",
        "    * weekday_cos, weekday_sin: cos и sin от weekday ($T = 7$).\n",
        "\n",
        "    * month_cos, month_sin: cos и sin от month ($T = 12$).\n",
        "\n",
        "Разделите датасет на обучающую и тестовую выборки так, чтобы в обучающую выборку вошли все данные ранее 2017 года, в тестовую — все данные за 2017 год и позже.\n",
        "\n",
        "Масштабируйте (стандартизируйте) все признаки на обучающей и тестовой выборке.\n",
        "\n",
        "**На обучающей выборке** подберите оптимальные гиперпараметры обучения линейной регрессии с L2-регуляризацией (Ridge) с помощью **кросс-валидации временных рядов** (TimeSeriesSplit и GridSearchCV), метрика оптимизации — 'neg_mean_squared_error' (**отрицательный MSE**). Рассчитайте лучшее среднее значение **RMSE** (Root Mean Square Error, среднеквадратическая ошибка) по результатам кросс-валидации.\n",
        "\n",
        "Обучите модель Ridge `ridge_aep` с оптимальными гиперпараметрами на всей обучающей выборке и выведите metrics_report **на тестовой выборке**.\n",
        "\n",
        "*Для данного задания предположим, что со временем среднее и дисперсия данных не изменились. Это позволит использовать один StandardScaler для обучающей и тестовой выборок.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae12fae-88ed-45bb-8bee-5c4388e2000d",
      "metadata": {
        "id": "fae12fae-88ed-45bb-8bee-5c4388e2000d"
      },
      "outputs": [],
      "source": [
        "# Считайте набор данных\n",
        "\n",
        "df_aep = pd.read_csv('AEP_hourly.csv')\n",
        "df_aep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6fba9b-6809-4115-b4ab-3345fffed84f",
      "metadata": {
        "id": "7b6fba9b-6809-4115-b4ab-3345fffed84f"
      },
      "outputs": [],
      "source": [
        "# Признак Datetime имеет тип данных object\n",
        "# Необходимо изменить тип данных для Datetime\n",
        "\n",
        "df_aep.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b625832e",
      "metadata": {
        "id": "b625832e"
      },
      "outputs": [],
      "source": [
        "# Измените тип Datetime на datetime64[ns]\n",
        "\n",
        "df_aep['Datetime'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb7f614",
      "metadata": {
        "id": "5bb7f614"
      },
      "outputs": [],
      "source": [
        "# Убедитесь, что тип данных Datetime — datetime64[ns]\n",
        "\n",
        "df_aep.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wPtcqhnc8MWj",
      "metadata": {
        "id": "wPtcqhnc8MWj"
      },
      "outputs": [],
      "source": [
        "# Постройте гистограмму потребления электроэнергии\n",
        "\n",
        "df_aep['AEP_MW'].hist(bins=100)\n",
        "plt.xlabel('Почасовое потребление электроэнергии, МВт')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdfb2668-c9f2-4c96-b557-c394878a286d",
      "metadata": {
        "id": "fdfb2668-c9f2-4c96-b557-c394878a286d"
      },
      "outputs": [],
      "source": [
        "# Постройте график потребления электроэнергии\n",
        "\n",
        "df_aep.plot(x='Datetime', y='AEP_MW', figsize=(12, 6))\n",
        "plt.xlabel('Время')\n",
        "plt.ylabel('Почасовое потребление электроэнергии, МВт')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57416f05",
      "metadata": {
        "id": "57416f05"
      },
      "outputs": [],
      "source": [
        "# Создайте признаки lag 1h и lag 24h\n",
        "# Подсказка: используйте shift\n",
        "\n",
        "df_aep['lag 1h'] = ...\n",
        "df_aep['lag 24h'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9aae2be",
      "metadata": {
        "id": "a9aae2be"
      },
      "outputs": [],
      "source": [
        "# Создайте признаки mean 24h и mean 7d\n",
        "# Подсказка: используйте признак lag 1h и rolling\n",
        "\n",
        "df_aep['mean 24h'] = ...\n",
        "df_aep['mean 7d'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a81c85-8246-4f22-bfe1-8b1cb3716ce3",
      "metadata": {
        "id": "d5a81c85-8246-4f22-bfe1-8b1cb3716ce3"
      },
      "outputs": [],
      "source": [
        "# Закодируйте метку времени (Datetime) как временные компоненты\n",
        "\n",
        "df_aep['hour'] = ...\n",
        "df_aep['weekday'] = ...\n",
        "df_aep['month'] = ...\n",
        "df_aep['day'] = ...\n",
        "df_aep['year'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb32202",
      "metadata": {
        "id": "8fb32202"
      },
      "outputs": [],
      "source": [
        "# Закодируйте метку времени (Datetime) как бинарный флаг is_weekend\n",
        "\n",
        "df_aep['is_weekend'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c3960b0",
      "metadata": {
        "id": "8c3960b0"
      },
      "outputs": [],
      "source": [
        "# Закодируйте циклические переменные с помощью косинуса и синуса\n",
        "\n",
        "df_aep['hour_cos'] = ...\n",
        "df_aep['hour_sin'] = ...\n",
        "\n",
        "df_aep['day_cos'] = ...\n",
        "df_aep['day_sin'] = ...\n",
        "\n",
        "df_aep['weekday_cos'] = ...\n",
        "df_aep['weekday_sin'] = ...\n",
        "\n",
        "df_aep['month_cos'] = ...\n",
        "df_aep['month_sin'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c206081",
      "metadata": {
        "id": "6c206081"
      },
      "outputs": [],
      "source": [
        "# Датасет после создания признаков\n",
        "\n",
        "df_aep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b52319",
      "metadata": {
        "id": "90b52319"
      },
      "outputs": [],
      "source": [
        "# Удалите строки с пропущенными значениями\n",
        "# Пропущенные значения появились после создания лаговых признаков и скользящих средних\n",
        "\n",
        "df_aep = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8978ae5f",
      "metadata": {
        "id": "8978ae5f"
      },
      "outputs": [],
      "source": [
        "# Выделите объясняемый фактор в отдельную переменную\n",
        "# Также удалите метку времени (Datetime) из объясняющих переменных\n",
        "\n",
        "X_aep, y_aep = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e7bc1d4",
      "metadata": {
        "id": "7e7bc1d4"
      },
      "outputs": [],
      "source": [
        "# Разделите датасет на обучающую и тестовую выборки:\n",
        "#   Обучающая выборка — все данные ранее 2017 года\n",
        "#   Тестовая выборка  — все данные за 2017 год и позже\n",
        "\n",
        "X_aep_train = ...\n",
        "y_aep_train = ...\n",
        "\n",
        "X_aep_test = ...\n",
        "y_aep_test = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89956c50",
      "metadata": {
        "id": "89956c50"
      },
      "outputs": [],
      "source": [
        "# Масштабируйте все признаки\n",
        "#   train -> fit_transform\n",
        "#   test -> transform\n",
        "\n",
        "aep_scaler = StandardScaler().set_output(transform='pandas')\n",
        "\n",
        "X_aep_train_scaled = ...\n",
        "X_aep_test_scaled = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e9a776",
      "metadata": {
        "id": "26e9a776"
      },
      "outputs": [],
      "source": [
        "# На обучающей выборке подберите оптимальные гиперпараметры обучения Ridge с помощью кросс-валидации временных рядов\n",
        "# Используйте TimeSeriesSplit и GridSearchCV\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "params = {\n",
        "    'alpha': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]\n",
        "}\n",
        "scoring = 'neg_mean_squared_error'\n",
        "cv = 10\n",
        "\n",
        "tscv = TimeSeriesSplit(...)\n",
        "\n",
        "cv_ridge_aep = GridSearchCV(\n",
        "    estimator=...,\n",
        "    param_grid=...,\n",
        "    scoring=...,\n",
        "    cv=...,\n",
        "    n_jobs=-1\n",
        ").fit(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a922a2",
      "metadata": {
        "id": "e2a922a2"
      },
      "outputs": [],
      "source": [
        "# Рассчитайте лучшее среднее значение RMSE по результатам кросс-валидации\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b9e5d3",
      "metadata": {
        "id": "b8b9e5d3"
      },
      "outputs": [],
      "source": [
        "# Обучите ridge_aep с оптимальными гиперпараметрами на всей обучающей выборке и выведите metrics_report на тестовой выборке\n",
        "\n",
        "ridge_aep = ...\n",
        "metrics_report(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "709e0680",
      "metadata": {
        "id": "709e0680"
      },
      "source": [
        "### **Отбор признаков**\n",
        "\n",
        "**Отбор признаков** (feature selection) — это процесс выбора наиболее информативных переменных из исходного набора данных. Такой подход позволяет упростить модель, сократить время обучения, повысить точность и уменьшить переобучение. Существует три основные группы методов: фильтры, обёртки и встроенные методы.\n",
        "\n",
        "**Фильтры (filter methods)**\n",
        "\n",
        "Фильтры оценивают важность признаков на основе статистических критериев независимо от используемой модели машинного обучения. Признаки оцениваются по отдельности, а затем выбирается подмножество признаков, которые лучше всего коррелируют с целевой переменной.\n",
        "\n",
        "Такие методы просты в реализации и быстры, но не учитывают взаимодействие между признаками и специфику модели.\n",
        "\n",
        "Примеры методов:\n",
        "\n",
        "* Корреляционный анализ.\n",
        "\n",
        "* Критерий $\\chi^2$ (хи-квадрат).\n",
        "\n",
        "* ANOVA F-тест.\n",
        "\n",
        "**Обёртки (wrapper methods)**\n",
        "\n",
        "Обёртки используют модели для оценки качества различных подмножеств признаков. Эти методы перебирают различные комбинации признаков, обучают модель и сравнивают качество прогнозов.\n",
        "\n",
        "Такие методы способны учитывать взаимодействие признаков и специфику модели, но вычислительно затратны.\n",
        "\n",
        "Примеры методов:\n",
        "\n",
        "* SFS (Sequential Forward Selection). Отбор начинается с пустого множества признаков. На каждом шаге к текущему набору добавляется тот признак, который при включении даёт наибольшее улучшение качества модели. Процесс продолжается до тех пор, когда не будет достигнут заданный размер подмножества или когда дальнейшее добавление признаков не будет улучшать качество.\n",
        "\n",
        "* SBS (Sequential Backward Selection). Отбор начинается со всех признаков. На каждом шаге удаляется тот признак, чьё исключение меньше всего ухудшает качество модели. Процесс повторяется, пока не останется заданное число признаков.\n",
        "\n",
        "* RFE (Recursive Feature Elimination). Отбор осуществляется на основе коэффициентов или важности признаков в обученной модели путем удаления наименее значимого признака. Процесс повторяется рекурсивно, пока не останется заданное количество признаков.\n",
        "\n",
        "**Встроенные методы (embedded methods)**\n",
        "\n",
        "Встроенные методы производят отбор признаков непосредственно в процессе обучения модели: используются внутренние механизмы модели для определения важности признаков.\n",
        "\n",
        "Встроенные методы сочетают преимущества фильтрующих (скорость) и оберточных (учет специфики модели) методов.\n",
        "\n",
        "Примеры методов:\n",
        "\n",
        "* L1-регуляризация (LASSO) для линейных моделей.\n",
        "\n",
        "* Feature Importance в деревьях решений и ансамблевых методах (Random Forest, Gradient Boosting).\n",
        "\n",
        "*На практике перечисленные методы могут комбинироваться. Например, сначала с помощью фильтра убираются малоинформативные признаки, а затем применяется обёртка для более точного отбора признаков.*\n",
        "\n",
        "Подробнее можно изучить по **ссылкам:**\n",
        "\n",
        "* [Comprehensive Guide on Feature Selection | kaggle.com](https://www.kaggle.com/code/prashant111/comprehensive-guide-on-feature-selection)\n",
        "\n",
        "* [Отбор признаков (Feature selection) | scikit-learn.ru](https://scikit-learn.ru/stable/modules/feature_selection.html#univariate-feature-selection)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d59d00da",
      "metadata": {
        "id": "d59d00da"
      },
      "source": [
        "### **Датасет *Company Bankruptcy Prediction***\n",
        "\n",
        "**Для решения задания 6 рассмотрим датасет [Company Bankruptcy Prediction](https://www.kaggle.com/datasets/fedesoriano/company-bankruptcy-prediction).**\n",
        "\n",
        "**ВНИМАНИЕ:** При решении задания **используйте файл bankruptcy.csv** из приложения к ноутбуку, поскольку исходный датасет был изменен авторами курса.\n",
        "\n",
        "Набор данных предназначен для выявления компаний с высоким риском банкротства на основе их финансовых показателей. Данные были собраны из базы данных компании Taiwan Economic Journal за период с 1999 по 2009 годы.\n",
        "\n",
        "Целевая переменная — Bankrupt? (банкротство компании):\n",
        "\n",
        "0 — в течение рассматриваемого периода компания работала без признаков банкротства.\n",
        "\n",
        "1 — компания обанкротилась в течение рассматриваемого периода.\n",
        "\n",
        "В датасете содержатся 95 различных показателей финансового состояния компаний, значения которых были предварительно стандартизированы.\n",
        "\n",
        "Одной из особенностей набора данных является дисбаланс классов в целевой переменной: только 3.22% от числа компаний в выборке были признаны банкротами за указанный период."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cbad130",
      "metadata": {
        "id": "6cbad130"
      },
      "source": [
        "### ***Задание 6***\n",
        "\n",
        "Используя все признаки, обучите baseline модель логистической регрессии (LogisticRegression) `lr_bankr_baseline` с параметрами:\n",
        "\n",
        "* solver='liblinear' — алгоритм оптимизации, который поддерживает L1 и L2 регуляризации.\n",
        "\n",
        "* class_weight='balanced' — корректировка веса классов обратно пропорционально их частотам в обучающем наборе данных.\n",
        "\n",
        "* random_state=RANDOM_STATE.\n",
        "\n",
        "*Baseline — это базовая модель, которая используется как отправная точка для оценки и сравнения более продвинутых моделей в задачах машинного обучения.*\n",
        "\n",
        "Выполните отбор признаков тремя методами и обучите модели на оптимальном наборе признаков:\n",
        "\n",
        "* **Фильтр: ANOVA F-тест (SelectKBest).**\n",
        "\n",
        "    1. Постройте пайплайн `bankr_kbest_pipeline`:\n",
        "\n",
        "        1. 'selector': [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)([f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)) — выбирает k лучших признаков на основе оценки ANOVA F-теста.\n",
        "\n",
        "        2. 'clf': LogisticRegression с теми же параметрами, что и baseline модель.\n",
        "\n",
        "    2. С помощью GridSearchCV и пайплайна `bankr_kbest_pipeline` подберите на обучающей выборке оптимальное количество признаков $k_{\\text{KBest}}$, где $k_{\\text{KBest}}$ — число от 1 до 40.\n",
        "\n",
        "    3. На оптимальном наборе из $k_{\\text{KBest}}$ признаков на всей обучающей выборке обучите модель (пайплайн) `lr_bankr_kbest` с теми же параметрами, что и baseline модель.\n",
        "\n",
        "* **Обёртка: SFS (SequentialFeatureSelector).**\n",
        "\n",
        "    1. Используя [SequentialFeatureSelector](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/#sequentialfeatureselector) из библиотеки mlxtend, подберите на обучающей выборке оптимальный набор из $k_{\\text{SFS}}$ признаков, где $k_{\\text{SFS}}$ — число от 1 до 40.\n",
        "\n",
        "    2. На оптимальном наборе из $k_{\\text{SFS}}$ признаков на всей обучающей выборке обучите модель `lr_bankr_sfs` с теми же параметрами, что и baseline модель.\n",
        "\n",
        "* **Встроенный метод: L1 регуляризация.**\n",
        "\n",
        "    1. С помощью GridSearchCV подберите на обучающей выборке значение оптимального гиперпараметра регуляризации C для **логистической регрессии с L1 регуляризацией** (остальные параметры те же, что и в baseline модели).\n",
        "\n",
        "    2. На всей обучающей выборке обучите модель с L1 регуляризацией `lr_bankr_l1`, в качестве параметров используя оптимальный гиперпараметр регуляризации и те же параметры, что и в baseline модели.\n",
        "\n",
        "Для обученных моделей `lr_bankr_kbest`, `lr_bankr_sfs` и `lr_bankr_l1` определите количество отобранных признаков k.\n",
        "\n",
        "На тестовой выборке для всех моделей, включая baseline (`lr_bankr`, `lr_bankr_kbest`, `lr_bankr_sfs` и `lr_bankr_l1`), постройте  отчёты по метрикам классификации, рассчитайте метрики f1 и AUC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae1c30b",
      "metadata": {
        "id": "2ae1c30b"
      },
      "outputs": [],
      "source": [
        "# Считайте данные\n",
        "\n",
        "df_bankr = pd.read_csv('bankruptcy.csv')\n",
        "df_bankr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "194067d7",
      "metadata": {
        "id": "194067d7"
      },
      "outputs": [],
      "source": [
        "# Всего в датасете 95 признаков (без учета целевой переменной)\n",
        "# Пропущенные значения отсутствуют\n",
        "\n",
        "df_bankr.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "634951c6",
      "metadata": {
        "id": "634951c6"
      },
      "outputs": [],
      "source": [
        "# В датасете присутствует дисбаланс классов\n",
        "\n",
        "df_bankr['Bankrupt?'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72907b3e",
      "metadata": {
        "id": "72907b3e"
      },
      "outputs": [],
      "source": [
        "# Выделите объясняемый фактор в отдельную переменную\n",
        "\n",
        "X_bankr, y_bankr = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "507a8578",
      "metadata": {
        "id": "507a8578"
      },
      "outputs": [],
      "source": [
        "# Разделите датасет на обучающую (60%) и тестовую (40%) выборки со стратификацией по целевой переменной\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "X_bankr_train, X_bankr_test, y_bankr_train, y_bankr_test = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ee7a1fd",
      "metadata": {
        "id": "0ee7a1fd"
      },
      "outputs": [],
      "source": [
        "# Обучите baseline модель lr_bankr_baseline\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "lr_bankr_baseline = LogisticRegression(random_state=RANDOM_STATE, solver='liblinear', class_weight='balanced')\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e51c4e",
      "metadata": {
        "id": "93e51c4e"
      },
      "outputs": [],
      "source": [
        "# Постройте отчет по метрикам классификации для модели lr_bankr на тестовой выборке\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c9fbe10",
      "metadata": {
        "id": "6c9fbe10"
      },
      "outputs": [],
      "source": [
        "# Посчитайте f1 для модели lr_bankr_baseline на тестовой выборке\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ca71c1",
      "metadata": {
        "id": "e2ca71c1"
      },
      "outputs": [],
      "source": [
        "# Посчитайте AUC для модели lr_bankr_baseline на тестовой выборке\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5cb9470",
      "metadata": {
        "id": "e5cb9470"
      },
      "outputs": [],
      "source": [
        "# Постройте пайплайн bankr_kbest_pipeline (см. задание)\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "bankr_kbest_pipeline = Pipeline([\n",
        "    ('selector', ...),\n",
        "    ('clf', ...)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837d1f72",
      "metadata": {
        "id": "837d1f72"
      },
      "outputs": [],
      "source": [
        "# С помощью GridSearchCV и bankr_kbest_pipeline подберите на обучающей выборке оптимальное количество признаков\n",
        "\n",
        "params = {\n",
        "    ...: range(1, 40)\n",
        "}\n",
        "scoring = 'f1'\n",
        "cv = 5\n",
        "\n",
        "cv_lr_bankr_kbest = GridSearchCV(\n",
        "    estimator=...,\n",
        "    param_grid=...,\n",
        "    scoring=...,\n",
        "    cv=...,\n",
        "    n_jobs=-1\n",
        ").fit(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aecdea1a",
      "metadata": {
        "id": "aecdea1a"
      },
      "outputs": [],
      "source": [
        "# Выведите оптимальное количество признаков для bankr_kbest_pipeline\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ae1ab3",
      "metadata": {
        "id": "92ae1ab3"
      },
      "outputs": [],
      "source": [
        "# На наборе из оптимального числа признаков на всей обучающей выборке обучите модель lr_bankr_kbest\n",
        "\n",
        "lr_bankr_kbest = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b48dd5ae",
      "metadata": {
        "id": "b48dd5ae"
      },
      "outputs": [],
      "source": [
        "# Постройте отчет по метрикам классификации для модели lr_bankr_kbest на тестовой выборке\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11e45b04",
      "metadata": {
        "id": "11e45b04"
      },
      "outputs": [],
      "source": [
        "# Посчитайте f1 для модели lr_bankr_kbest на тестовой выборке\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79d2fb9d",
      "metadata": {
        "id": "79d2fb9d"
      },
      "outputs": [],
      "source": [
        "# Посчитайте AUC для модели lr_bankr_kbest на тестовой выборке\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87db29d",
      "metadata": {
        "id": "b87db29d"
      },
      "outputs": [],
      "source": [
        "# Используя SequentialFeatureSelector, подберите на обучающей выборке оптимальный набор признаков\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "bankr_sfs = SequentialFeatureSelector(\n",
        "    estimator=...,\n",
        "    k_features=...,\n",
        "    forward=True,\n",
        "    floating=False,\n",
        "    scoring='f1',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ").fit(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be6fb255",
      "metadata": {
        "id": "be6fb255"
      },
      "outputs": [],
      "source": [
        "# Выведите оптимальное количество признаков после SFS\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69993580",
      "metadata": {
        "id": "69993580"
      },
      "outputs": [],
      "source": [
        "# Создайте новые выборки с оптимальными признаками после SFS\n",
        "\n",
        "X_bankr_train_sfs = ...\n",
        "X_bankr_test_sfs = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c38920c9",
      "metadata": {
        "id": "c38920c9"
      },
      "outputs": [],
      "source": [
        "# На оптимальном наборе признаков на всей обучающей выборке обучите модель lr_bankr_sfs\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "lr_bankr_sfs = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd8e6d84",
      "metadata": {
        "id": "dd8e6d84"
      },
      "outputs": [],
      "source": [
        "# Постройте отчет по метрикам классификации для модели lr_bankr_sfs на тестовой выборке\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f375b22",
      "metadata": {
        "id": "8f375b22"
      },
      "outputs": [],
      "source": [
        "# Посчитайте f1 для модели lr_bankr_sfs на тестовой выборке\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f350e76",
      "metadata": {
        "id": "7f350e76"
      },
      "outputs": [],
      "source": [
        "# Посчитайте AUC для модели lr_bankr_sfs на тестовой выборке\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adc4e831",
      "metadata": {
        "id": "adc4e831"
      },
      "outputs": [],
      "source": [
        "# С помощью GridSearchCV подберите на обучающей выборке значение оптимального гиперпараметра регуляризации C\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "estimator = LogisticRegression(..., penalty='l1')\n",
        "params = {'C' : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]}\n",
        "cv = 5\n",
        "scoring = 'f1'\n",
        "\n",
        "cv_lr_bankr_l1 = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e280426",
      "metadata": {
        "id": "2e280426"
      },
      "outputs": [],
      "source": [
        "# На всей обучающей выборке обучите модель с L1 регуляризацией lr_bankr_l1\n",
        "# Не забудьте зафиксировать RANDOM_STATE\n",
        "\n",
        "lr_bankr_l1 = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b65b153",
      "metadata": {
        "id": "4b65b153"
      },
      "outputs": [],
      "source": [
        "# Выведите количество отобранных признаков (признаков, регрессионный коэффициент при которых не равен 0) в lr_bankr_l1\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02278dfd",
      "metadata": {
        "id": "02278dfd"
      },
      "outputs": [],
      "source": [
        "# Постройте отчет по метрикам классификации для модели lr_bankr_l1 на тестовой выборке\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e4a633",
      "metadata": {
        "id": "e4e4a633"
      },
      "outputs": [],
      "source": [
        "# Посчитайте f1 для модели lr_bankr_l1 на тестовой выборке\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ecfeecf",
      "metadata": {
        "id": "2ecfeecf"
      },
      "outputs": [],
      "source": [
        "# Посчитайте AUC для модели lr_bankr_l1 на тестовой выборке\n",
        "\n",
        "..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "ml-course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}